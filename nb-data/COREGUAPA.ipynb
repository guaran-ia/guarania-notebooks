{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4Su9qcjcJJrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install fasttext nltk langcodes\n",
        "\n",
        "# Download the FastText language identification model (lid.176.bin)\n",
        "!wget -O lid.176.bin https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin"
      ],
      "metadata": {
        "id": "zDfIMvB6Ns46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c403a8-7c67-4db9-b35c-b48a73a70781"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: langcodes in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (1.26.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes) (1.2.1)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313472 sha256=bb900d05fbb3612518ba70da0306b833cd25fc91dd0aaff60841f3e1b41a0565\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n",
            "--2025-02-18 09:37:44--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.167.112.129, 3.167.112.66, 3.167.112.51, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.167.112.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 131266198 (125M) [application/octet-stream]\n",
            "Saving to: ‘lid.176.bin’\n",
            "\n",
            "lid.176.bin         100%[===================>] 125.18M   295MB/s    in 0.4s    \n",
            "\n",
            "2025-02-18 09:37:44 (295 MB/s) - ‘lid.176.bin’ saved [131266198/131266198]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "import os\n",
        "import fasttext\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import collections\n",
        "import string\n",
        "from langcodes import *\n",
        "\n",
        "# Download NLTK's Punkt tokenizer models\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Mount Google Drive to access your text files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the path to your folder in Google Drive (update the path as needed)\n",
        "folder_path = '/content/drive/MyDrive/Research/BID-GuaranIA/TXT COREGUAPA'\n",
        "\n",
        "# Read all .txt files from the folder and combine their contents\n",
        "all_text = \"\"\n",
        "file_names = []\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "            all_text += text + \"\\n\"\n",
        "            file_names.append(filename)\n",
        "\n",
        "print(\"Processed files:\", file_names)\n",
        "print(\"Total Processed files:\", len(file_names))\n",
        "# Tokenize the combined text into sentences and words.\n",
        "# Using the Spanish sentence tokenizer since punctuation is similar;\n",
        "# adjust if necessary for your specific text.\n",
        "sentences = nltk.sent_tokenize(all_text, language='spanish')\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    return text\n",
        "all_text = clean_text(all_text)\n",
        "\n",
        "tokens_a = nltk.word_tokenize(all_text)\n",
        "tt = TweetTokenizer()\n",
        "tokens_b = tt.tokenize(all_text)\n",
        "tokens_c = all_text.replace('\\n', ' ').strip().split()\n",
        "\n",
        "total_tokens_a = len(tokens_a)\n",
        "total_tokens_b = len(tokens_b)\n",
        "total_tokens_c = len(tokens_c)\n",
        "total_sentences = len(sentences)\n",
        "\n",
        "print(f\"Total tokens method A: {total_tokens_a}\")\n",
        "print(f\"Total tokens method B: {total_tokens_b}\")\n",
        "print(f\"Total tokens method C: {total_tokens_c}\")\n",
        "print(f\"Total sentences: {total_sentences}\")\n",
        "\n",
        "# Compute additional statistics\n",
        "token_freq = collections.Counter(tokens_c)\n",
        "vocab_size = len(token_freq)\n",
        "avg_sentence_length = total_tokens_c / total_sentences if total_sentences > 0 else 0\n",
        "\n",
        "print(f\"Vocabulary size (unique tokens): {vocab_size}\")\n",
        "print(f\"Average sentence length (in tokens): {avg_sentence_length:.2f}\")\n",
        "\n",
        "# Load FastText language identification model\n",
        "ft_model = fasttext.load_model('lid.176.bin')\n",
        "\n",
        "# Initialize counters for language-based word counts\n",
        "spanish_word_count = 0\n",
        "guarani_word_count = 0\n",
        "other_word_count = {}\n",
        "spanish_sentence_count = 0\n",
        "guarani_sentence_count = 0\n",
        "other_sentence_count = {}\n",
        "\n",
        "def return_gn(ft_prediction):\n",
        "    ft_prediction = ft_prediction[0]\n",
        "    # Example label format: '__label__es' for Spanish.\n",
        "    if '__label__gn' in ft_prediction:\n",
        "        return 'gn'\n",
        "    elif '__label__es' in ft_prediction:\n",
        "        return 'es'\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Process each sentence:\n",
        "# - Predict the language using FastText.\n",
        "# - Tokenize the sentence into words.\n",
        "# - Aggregate counts based on the predicted language.\n",
        "for sentence in sentences:\n",
        "    # Clean sentence (remove extra newlines/spaces)\n",
        "    sentence_clean = clean_text(sentence).replace('\\n', ' ').strip()\n",
        "    if not sentence_clean:\n",
        "        continue\n",
        "    # FastText returns a tuple with predicted label(s) and probabilities.\n",
        "    prediction = ft_model.predict(sentence_clean, k=5)\n",
        "\n",
        "    lang_code = return_gn(prediction)\n",
        "    #words_in_sentence = nltk.word_tokenize(sentence_clean)\n",
        "    #words_in_sentence = tt.tokenize(sentence_clean)\n",
        "    words_in_sentence = sentence_clean.split()\n",
        "    if lang_code == 'es':\n",
        "        spanish_word_count += len(words_in_sentence)\n",
        "        spanish_sentence_count += 1\n",
        "    elif lang_code == 'gn':  # Assuming 'gn' is returned for Guarani\n",
        "        guarani_word_count += len(words_in_sentence)\n",
        "        guarani_sentence_count += 1\n",
        "    else:\n",
        "        lang_label = prediction[0][0]\n",
        "        lang_code = lang_label.replace('__label__', '')\n",
        "        other_word_count[lang_code] = other_word_count.get(lang_code, 0) + len(words_in_sentence)\n",
        "        other_sentence_count[lang_code] = other_sentence_count.get(lang_code, 0) + 1\n",
        "\n",
        "print(\"\\nLanguage-based token counts (by sentence prediction):\")\n",
        "print(f\"Spanish -> tokens: {spanish_word_count}, sentences: {spanish_sentence_count}\")\n",
        "print(f\"Guarani -> tokens: {guarani_word_count}, sentences: {guarani_sentence_count}\")\n",
        "print(f\"Other/unknown languages -> tokens: {sum(other_word_count.values())}, sentences: {sum(other_sentence_count.values())}\")\n",
        "print(\"Top 10 most common other languages:\")\n",
        "for language, freq in  collections.Counter(other_sentence_count).most_common(10):\n",
        "    lang_name =  Language.make(language=language).display_name()\n",
        "    print(f\"{lang_name}: {freq}\")\n",
        "\n",
        "# Additional insight: Display the 20 most common tokens\n",
        "print(\"\\nTop 10 most common tokens:\")\n",
        "for token, freq in token_freq.most_common(10):\n",
        "    print(f\"{token}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EimPVT0JOWO",
        "outputId": "7476a8cd-519e-40d6-e96e-e67e6affb117"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Processed files: ['Paraguái Léi Guasu ary 1992.txt', 'LÉI PY 5446 TETÃ REMBIAPORÃITE KUÑA OKARAYGUÁPE G̃UARÃ.txt', 'TEMBIAPOUKAPY PY 2991 MBOJOAPY.txt', 'LÉI Ppy 6530.txt', 'LÉI PY 5777.txt', 'LÉI PY 5016-14.txt', 'LÉI PAPAPY 1334-98.txt', 'Léi 4251-10 Ñe’enguéra Rehegua.txt', 'PARAGUÁI CÓDIGO ELECTORAL.txt', 'Ñañangareko hag̃ua ñande rekoha rehe Ciencias Naturales 6º Grado EEB.txt', 'Ñane ñe’ẽtee Lengua Materna 5° Grado EEB.txt', 'Ñandekatupyry hag̃ua papapykuérape Matematica 6º Grado EEB.txt', 'Jaguerojera hag̃ua mba’e porã Educación Artística 7º Grado EEB.txt', 'Ñañangareko porãve hag̃ua ñande rete rehe Educación Física 7º grado EEB.txt', 'Ñañangareko porãve hag̃ua ñande rete rehe Educación Física 7º grado EEB (1).txt', 'Teratee ojeporúva Paraguái Retäme.txt', 'Jaguerojera hag̃ua mba’e porã Educ. Artística 6º Grado EEB.txt', 'Jaguerojera hag̃ua mba’e porã Educación artística 5º Grado EEB.txt', 'Jajogueraha porãve hag̃ua oñondive. Ciencias Sociales 5º Grado EEB.txt', 'Ñandekatupyry hag̃ua papapykuérape Matemática 2º Grado EEB.txt', 'Ñane Ñe’ẽjoapy Segunda Lengua 5º Grado EEB.txt', 'Ñane Ñe’ẽjoapy Segunda Lengua 6º Grado EEB.txt', 'Jajogueraha porãve hag̃ua oñondive Ciencias Sociales 4º Grado EEB.txt', 'Jaguerojera hag̃ua mba’e porã Educación Artísitica 4º Grado EEB.txt', 'Ñañangareko porãve hag̃ua ñande rete rehe Educación para la Salud y Educación Física 4º Grado EEB.txt', 'Ñane ñe’ẽjoapy Segunda Lengua 4º Grado EEB.txt', 'Ñandekatupyryve hag̃ua ñane rembiapópe Trabajo y Tecnología 4º Grado EEB.txt', 'Ñandekatupyry hag̃ua papapykuérape Matemática 4º Grado EEB.txt', 'Ñandekatupyryve hag̃ua ñane rembiapópe Trabajo y Tecnología 5º Grado EEB.txt', 'Ñandekatupyry hag̃ua papapykuérape Matemática 5º Grado EEB.txt', 'Jajogueraha porãve hag̃ua oñondive Ciencias Sociales 6º Grado EEB.txt', 'Ñandekatupyryve hag̃ua ñane rembiapópe Trabajo y Tecnología 6º Grado EEB.txt', 'Momarandu Comunicación 2º Grado EEB.txt', 'Momarandu - Comunicación 1º Grado EEB.txt', 'Momarandu Comunicación 3º Grado EEB.txt', 'Ñane ñe’ẽtee Lengua Materna 4º Grado EEB.txt', 'Ñañangarekokuaa hag̃ua ñande rekoha ha ñande rete rehe Medio Natural y Salud 2º Grado EEB.txt', 'Ñandekatupyry hag̃ua papapykuérape Matemática 1º Grado EEB.txt', 'Ñandekatupyry hag̃ua papapykuérape Matemática 3º Grado EEB.txt', 'Ñañangarekokuaa hag̃ua ñande rekoha ha ñande rete rehe Medio Natural y Salud 3º Grado EEB.txt', 'Jaikokuaa hag̃ua oñondive ha ñamba’apokuaa hag̃ua Vida Social y Trabajo 3º Grado EEB.txt', 'Mombe’upyre. Mombe’upyrã. Káso ñemombe’u.txt', 'Ñe’ẽnga II. Dichos populares paraguayos.txt', 'Ñe’ẽnga. Dichos populares del Paraguay.txt', 'Ñe’ẽnga. Dichos populares paraguayos.txt', 'Pukarã. Chistes folclóricos paraguayos. Primera parte.txt', 'Purahéi.txt', 'Tataypýpe. Junto al fuego. Next to the fire.txt', 'Tetãnguéra leiguasu mitã derecho rehegua.txt', 'Tupã oñe’ẽ ita’yrakuérape. Kuatiañe’ẽ Tupã Ñe’ẽ rehegua.txt', 'NeengaIDA.txt', 'Kokueguára rembiasa. Tomo I.txt', 'Folklore paraguayo. Selección de mitos, leyendas, fábulas y costumbres.txt', 'Ayvu membyre. Hijo de aquel verbo.txt', 'Diccionario koygua. Sinónimos informales en guaraní.txt', 'Kokueguára rembiasa. Tomo II.txt', 'Kokueguára rembiasa. Tomo III.txt', 'Kokueguára rembiasa. Tomo IV.txt', \"Kunu'ũ Roky.txt\", 'Las cien mejores poesías en guaraní.txt', 'Maravichu maravichu ha kũjererã. Adivinanzas y trabalenguas del Paraguay.txt']\n",
            "Total Processed files: 61\n",
            "Total tokens method A: 790177\n",
            "Total tokens method B: 793213\n",
            "Total tokens method C: 632485\n",
            "Total sentences: 60421\n",
            "Vocabulary size (unique tokens): 76551\n",
            "Average sentence length (in tokens): 10.47\n",
            "\n",
            "Language-based token counts (by sentence prediction):\n",
            "Spanish -> tokens: 98614, sentences: 13102\n",
            "Guarani -> tokens: 504493, sentences: 38595\n",
            "Other/unknown languages -> tokens: 29379, sentences: 8722\n",
            "Top 10 most common other languages:\n",
            "French: 1672\n",
            "German: 883\n",
            "Swedish: 740\n",
            "English: 659\n",
            "Hungarian: 502\n",
            "Italian: 452\n",
            "Portuguese: 318\n",
            "Croatian: 301\n",
            "Czech: 289\n",
            "Finnish: 207\n",
            "\n",
            "Top 10 most common tokens:\n",
            "ha: 27318\n",
            "la: 9045\n",
            "hag̃ua: 7291\n",
            "pe: 7191\n",
            "umi: 6657\n",
            "che: 6601\n",
            "peteĩ: 5330\n",
            "mba’e: 4633\n",
            "avei: 4321\n",
            "de: 3958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# '/content/drive/MyDrive/Research/BID-GuaranIA/TXT COREGUAPA'"
      ],
      "metadata": {
        "id": "adFQ6jvaUIx7"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}