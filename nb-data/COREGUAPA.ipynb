{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4Su9qcjcJJrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install fasttext nltk langcodes huggingface_hub\n",
        "\n",
        "# Download the FastText language identification model (lid.176.bin)\n",
        "!wget -O lid.176.bin https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin"
      ],
      "metadata": {
        "id": "zDfIMvB6Ns46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f1a229-8a71-425b-92d7-ef0458fa562c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: langcodes in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (1.26.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes) (1.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes) (1.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313464 sha256=41fc95f65ba966eb656a073daee2b6ecc307c7fcb650c11c1fe019ec7e22bfff\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n",
            "--2025-02-18 16:38:53--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.96, 3.163.189.51, 3.163.189.14, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 131266198 (125M) [application/octet-stream]\n",
            "Saving to: ‘lid.176.bin’\n",
            "\n",
            "lid.176.bin         100%[===================>] 125.18M   192MB/s    in 0.7s    \n",
            "\n",
            "2025-02-18 16:38:54 (192 MB/s) - ‘lid.176.bin’ saved [131266198/131266198]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "import numpy as np\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "class CustomLID:\n",
        "    def __init__(self, model_path, languages = -1, mode='before'):\n",
        "        self.model = fasttext.load_model(model_path)\n",
        "        self.output_matrix = self.model.get_output_matrix()\n",
        "        self.labels = self.model.get_labels()\n",
        "\n",
        "        # compute language_indices\n",
        "        if languages !=-1 and isinstance(languages, list):\n",
        "            self.language_indices = [self.labels.index(l) for l in list(set(languages)) if l in self.labels]\n",
        "\n",
        "        else:\n",
        "            self.language_indices = list(range(len(self.labels)))\n",
        "\n",
        "        # limit labels to language_indices\n",
        "        self.labels = list(np.array(self.labels)[self.language_indices])\n",
        "\n",
        "        # predict\n",
        "        self.predict = self.predict_limit_after_softmax if mode=='after' else self.predict_limit_before_softmax\n",
        "\n",
        "\n",
        "    def predict_limit_before_softmax(self, text, k=1):\n",
        "\n",
        "        # sentence vector\n",
        "        sentence_vector = self.model.get_sentence_vector(text)\n",
        "\n",
        "        # dot\n",
        "        result_vector = np.dot(self.output_matrix[self.language_indices, :], sentence_vector)\n",
        "\n",
        "        # softmax\n",
        "        softmax_result = np.exp(result_vector - np.max(result_vector)) / np.sum(np.exp(result_vector - np.max(result_vector)))\n",
        "\n",
        "        # top k predictions\n",
        "        top_k_indices = np.argsort(softmax_result)[-k:][::-1]\n",
        "        top_k_labels = [self.labels[i] for i in top_k_indices]\n",
        "        top_k_probs = softmax_result[top_k_indices]\n",
        "\n",
        "        return tuple(top_k_labels), top_k_probs\n",
        "\n",
        "\n",
        "    def predict_limit_after_softmax(self, text, k=1):\n",
        "\n",
        "        # sentence vector\n",
        "        sentence_vector = self.model.get_sentence_vector(text)\n",
        "\n",
        "        # dot\n",
        "        result_vector = np.dot(self.output_matrix, sentence_vector)\n",
        "\n",
        "        # softmax\n",
        "        softmax_result = np.exp(result_vector - np.max(result_vector)) / np.sum(np.exp(result_vector - np.max(result_vector)))\n",
        "\n",
        "        # limit softmax to language_indices\n",
        "        softmax_result = softmax_result[self.language_indices]\n",
        "\n",
        "\n",
        "        # top k predictions\n",
        "        top_k_indices = np.argsort(softmax_result)[-k:][::-1]\n",
        "        top_k_labels = [self.labels[i] for i in top_k_indices]\n",
        "        top_k_probs = softmax_result[top_k_indices]\n",
        "\n",
        "        return tuple(top_k_labels), top_k_probs\n",
        "\n",
        "\n",
        "\n",
        "# download model\n",
        "## cache_dir: path to the folder where the downloaded model will be stored/cached.\n",
        "model_path = hf_hub_download(repo_id=\"cis-lmu/glotlid\", filename=\"model.bin\", cache_dir=None)\n",
        "\n",
        "\n",
        "# to make sure these languages are available in GlotLID check the list of supported labels in model.labels\n",
        "limited_languages = ['__label__eng_Latn', '__label__arb_Arab', '__label__rus_Cyrl', '__label__por_Latn', '__label__pol_Latn', '__label__hin_Deva']\n",
        "\n",
        "model = CustomLID(model_path, languages = limited_languages , mode='before')\n",
        "\n",
        "p = model.predict(\"Hello, world!\", 3)\n",
        "print(p, '\\n', p[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTgMtAgP-2md",
        "outputId": "8e9684b0-d0e6-4e7a-a60f-bd55fca14723"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(('__label__eng_Latn', '__label__pol_Latn', '__label__hin_Deva'), array([9.9995399e-01, 4.2381838e-05, 3.2946982e-06], dtype=float32)) \n",
            " __label__eng_Latn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "def return_gn(ft_prediction, tool='fasttext'):\n",
        "    if tool == 'fasttext':\n",
        "      ft_prediction = ft_prediction[0]\n",
        "      # Example label format: '__label__es' for Spanish.\n",
        "      if '__label__gn' in ft_prediction:\n",
        "          return 'gn'\n",
        "      elif '__label__es' in ft_prediction:\n",
        "          return 'es'\n",
        "      else:\n",
        "          return None\n",
        "    elif tool == 'glotlid':\n",
        "      if any([x for x in ['__label__gui_Latn', '__label__gug_Latn', '__label__grn_Latn', '__label__gnw_Latn', '__label__gun_Latn'] if x == ft_prediction[0][0]]):\n",
        "          return 'gn'\n",
        "      elif '__label__spa_Latn' in ft_prediction[0]:\n",
        "          return 'es'\n",
        "      else:\n",
        "          return None"
      ],
      "metadata": {
        "id": "HrLgsdIy_ClV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "import os, sys\n",
        "import fasttext\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import collections\n",
        "import string\n",
        "from langcodes import *\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Download NLTK's Punkt tokenizer models\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# download model and get the model path\n",
        "# cache_dir is the path to the folder where the downloaded model will be stored/cached.\n",
        "model_path = hf_hub_download(repo_id=\"cis-lmu/glotlid\", filename=\"model.bin\", cache_dir=None)\n",
        "print(\"LID model path:\", model_path)\n",
        "\n",
        "# Mount Google Drive to access your text files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the path to your folder in Google Drive (update the path as needed)\n",
        "folder_path = '/content/drive/MyDrive/Research/BID-GuaranIA/TXT COREGUAPA'\n",
        "\n",
        "# Read all .txt files from the folder and combine their contents\n",
        "all_text = \"\"\n",
        "file_names = []\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "            all_text += text + \"\\n\"\n",
        "            file_names.append(filename)\n",
        "\n",
        "print(\"Processed files:\", file_names)\n",
        "print(\"Total Processed files:\", len(file_names))\n",
        "# Tokenize the combined text into sentences and words.\n",
        "# Using the Spanish sentence tokenizer since punctuation is similar;\n",
        "# adjust if necessary for your specific text.\n",
        "sentences = nltk.sent_tokenize(all_text, language='spanish')\n",
        "\n",
        "# clean before tokenize\n",
        "all_text = clean_text(all_text)\n",
        "\n",
        "tokens_a = nltk.word_tokenize(all_text)\n",
        "tt = TweetTokenizer()\n",
        "tokens_b = tt.tokenize(all_text)\n",
        "tokens_c = all_text.replace('\\n', ' ').strip().split()\n",
        "\n",
        "total_tokens_a = len(tokens_a)\n",
        "total_tokens_b = len(tokens_b)\n",
        "total_tokens_c = len(tokens_c)\n",
        "total_sentences = len(sentences)\n",
        "\n",
        "print(f\"Total tokens method A: {total_tokens_a}\")\n",
        "print(f\"Total tokens method B: {total_tokens_b}\")\n",
        "print(f\"Total tokens method C: {total_tokens_c}\")\n",
        "print(f\"Total sentences: {total_sentences}\")\n",
        "\n",
        "# Compute additional statistics\n",
        "token_freq = collections.Counter(tokens_c)\n",
        "vocab_size = len(token_freq)\n",
        "avg_sentence_length = total_tokens_c / total_sentences if total_sentences > 0 else 0\n",
        "\n",
        "print(f\"Vocabulary size (unique tokens): {vocab_size}\")\n",
        "print(f\"Average sentence length (in tokens): {avg_sentence_length:.2f}\")\n",
        "\n",
        "# Load FastText language identification model\n",
        "ft_model = fasttext.load_model('lid.176.bin')\n",
        "# to make sure these languages are available in GlotLID check the list of supported labels in model.labels\n",
        "limited_languages = [\n",
        "    '__label__gui_Latn', '__label__gug_Latn', '__label__grn_Latn', '__label__gnw_Latn', '__label__gun_Latn',\n",
        "    '__label__eng_Latn', '__label__spa_Latn', '__label__por_Latn',\n",
        "]\n",
        "model = CustomLID(model_path, languages = limited_languages , mode='before')\n",
        "\n",
        "# Initialize counters for language-based word counts\n",
        "spanish_word_count = 0\n",
        "guarani_word_count = 0\n",
        "other_word_count = {}\n",
        "spanish_sentence_count = 0\n",
        "guarani_sentence_count = 0\n",
        "other_sentence_count = {}\n",
        "\n",
        "# Process each sentence:\n",
        "# - Predict the language using FastText/GlotLID.\n",
        "# - Tokenize the sentence into words.\n",
        "# - Aggregate counts based on the predicted language.\n",
        "for sentence in sentences:\n",
        "    # Clean sentence (remove extra newlines/spaces)\n",
        "    sentence_clean = clean_text(sentence).replace('\\n', ' ').strip()\n",
        "    if not sentence_clean:\n",
        "        continue\n",
        "    # FastText returns a tuple with predicted label(s) and probabilities.\n",
        "    #prediction = ft_model.predict(sentence_clean, k=5)\n",
        "    prediction = model.predict(sentence_clean, k=5)\n",
        "\n",
        "    # is guarani?\n",
        "    lang_code = return_gn(prediction, tool='glotlid')\n",
        "    #words_in_sentence = nltk.word_tokenize(sentence_clean)\n",
        "    #words_in_sentence = tt.tokenize(sentence_clean)\n",
        "    words_in_sentence = sentence_clean.split()\n",
        "    if lang_code == 'es':\n",
        "        spanish_word_count += len(words_in_sentence)\n",
        "        spanish_sentence_count += 1\n",
        "    elif lang_code == 'gn':  # Assuming 'gn' is returned for Guarani\n",
        "        guarani_word_count += len(words_in_sentence)\n",
        "        guarani_sentence_count += 1\n",
        "    else:\n",
        "        lang_label = prediction[0][0]\n",
        "        lang_code = lang_label.replace('__label__', '').replace('_Latn', '')\n",
        "        other_word_count[lang_code] = other_word_count.get(lang_code, 0) + len(words_in_sentence)\n",
        "        other_sentence_count[lang_code] = other_sentence_count.get(lang_code, 0) + 1\n",
        "\n",
        "print(\"\\nLanguage-based token counts (by sentence prediction):\")\n",
        "print(f\"Spanish -> tokens: {spanish_word_count}, sentences: {spanish_sentence_count}\")\n",
        "print(f\"Guarani -> tokens: {guarani_word_count}, sentences: {guarani_sentence_count}\")\n",
        "print(f\"Other/unknown languages -> tokens: {sum(other_word_count.values())}, sentences: {sum(other_sentence_count.values())}\")\n",
        "print(\"Top 10 most common other languages:\")\n",
        "for language, freq in  collections.Counter(other_sentence_count).most_common(10):\n",
        "    lang_name =  Language.make(language=language).display_name()\n",
        "    print(f\"{lang_name}: {freq}\")\n",
        "\n",
        "# Additional insight: Display the 20 most common tokens\n",
        "print(\"\\nTop 10 most common tokens:\")\n",
        "for token, freq in token_freq.most_common(10):\n",
        "    print(f\"{token}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EimPVT0JOWO",
        "outputId": "72518965-e64d-4c5a-c124-3e0e77c283ac"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LID model path: /root/.cache/huggingface/hub/models--cis-lmu--glotlid/snapshots/74cb50b709c9eefe0f790030c6c95c461b4e3b77/model.bin\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Processed files: ['Paraguái Léi Guasu ary 1992.txt', 'LÉI PY 5446 TETÃ REMBIAPORÃITE KUÑA OKARAYGUÁPE G̃UARÃ.txt', 'TEMBIAPOUKAPY PY 2991 MBOJOAPY.txt', 'LÉI Ppy 6530.txt', 'LÉI PY 5777.txt', 'LÉI PY 5016-14.txt', 'LÉI PAPAPY 1334-98.txt', 'Léi 4251-10 Ñe’enguéra Rehegua.txt', 'PARAGUÁI CÓDIGO ELECTORAL.txt', 'Ñañangareko hag̃ua ñande rekoha rehe Ciencias Naturales 6º Grado EEB.txt', 'Ñane ñe’ẽtee Lengua Materna 5° Grado EEB.txt', 'Ñandekatupyry hag̃ua papapykuérape Matematica 6º Grado EEB.txt', 'Jaguerojera hag̃ua mba’e porã Educación Artística 7º Grado EEB.txt', 'Ñañangareko porãve hag̃ua ñande rete rehe Educación Física 7º grado EEB.txt', 'Ñañangareko porãve hag̃ua ñande rete rehe Educación Física 7º grado EEB (1).txt', 'Teratee ojeporúva Paraguái Retäme.txt', 'Jaguerojera hag̃ua mba’e porã Educ. Artística 6º Grado EEB.txt', 'Jaguerojera hag̃ua mba’e porã Educación artística 5º Grado EEB.txt', 'Jajogueraha porãve hag̃ua oñondive. Ciencias Sociales 5º Grado EEB.txt', 'Ñandekatupyry hag̃ua papapykuérape Matemática 2º Grado EEB.txt', 'Ñane Ñe’ẽjoapy Segunda Lengua 5º Grado EEB.txt', 'Ñane Ñe’ẽjoapy Segunda Lengua 6º Grado EEB.txt', 'Jajogueraha porãve hag̃ua oñondive Ciencias Sociales 4º Grado EEB.txt', 'Jaguerojera hag̃ua mba’e porã Educación Artísitica 4º Grado EEB.txt', 'Ñañangareko porãve hag̃ua ñande rete rehe Educación para la Salud y Educación Física 4º Grado EEB.txt', 'Ñane ñe’ẽjoapy Segunda Lengua 4º Grado EEB.txt', 'Ñandekatupyryve hag̃ua ñane rembiapópe Trabajo y Tecnología 4º Grado EEB.txt', 'Ñandekatupyry hag̃ua papapykuérape Matemática 4º Grado EEB.txt', 'Ñandekatupyryve hag̃ua ñane rembiapópe Trabajo y Tecnología 5º Grado EEB.txt', 'Ñandekatupyry hag̃ua papapykuérape Matemática 5º Grado EEB.txt', 'Jajogueraha porãve hag̃ua oñondive Ciencias Sociales 6º Grado EEB.txt', 'Ñandekatupyryve hag̃ua ñane rembiapópe Trabajo y Tecnología 6º Grado EEB.txt', 'Momarandu Comunicación 2º Grado EEB.txt', 'Momarandu - Comunicación 1º Grado EEB.txt', 'Momarandu Comunicación 3º Grado EEB.txt', 'Ñane ñe’ẽtee Lengua Materna 4º Grado EEB.txt', 'Ñañangarekokuaa hag̃ua ñande rekoha ha ñande rete rehe Medio Natural y Salud 2º Grado EEB.txt', 'Ñandekatupyry hag̃ua papapykuérape Matemática 1º Grado EEB.txt', 'Ñandekatupyry hag̃ua papapykuérape Matemática 3º Grado EEB.txt', 'Ñañangarekokuaa hag̃ua ñande rekoha ha ñande rete rehe Medio Natural y Salud 3º Grado EEB.txt', 'Jaikokuaa hag̃ua oñondive ha ñamba’apokuaa hag̃ua Vida Social y Trabajo 3º Grado EEB.txt', 'Mombe’upyre. Mombe’upyrã. Káso ñemombe’u.txt', 'Ñe’ẽnga II. Dichos populares paraguayos.txt', 'Ñe’ẽnga. Dichos populares del Paraguay.txt', 'Ñe’ẽnga. Dichos populares paraguayos.txt', 'Pukarã. Chistes folclóricos paraguayos. Primera parte.txt', 'Purahéi.txt', 'Tataypýpe. Junto al fuego. Next to the fire.txt', 'Tetãnguéra leiguasu mitã derecho rehegua.txt', 'Tupã oñe’ẽ ita’yrakuérape. Kuatiañe’ẽ Tupã Ñe’ẽ rehegua.txt', 'NeengaIDA.txt', 'Kokueguára rembiasa. Tomo I.txt', 'Folklore paraguayo. Selección de mitos, leyendas, fábulas y costumbres.txt', 'Ayvu membyre. Hijo de aquel verbo.txt', 'Diccionario koygua. Sinónimos informales en guaraní.txt', 'Kokueguára rembiasa. Tomo II.txt', 'Kokueguára rembiasa. Tomo III.txt', 'Kokueguára rembiasa. Tomo IV.txt', \"Kunu'ũ Roky.txt\", 'Las cien mejores poesías en guaraní.txt', 'Maravichu maravichu ha kũjererã. Adivinanzas y trabalenguas del Paraguay.txt']\n",
            "Total Processed files: 61\n",
            "Total tokens method A: 790177\n",
            "Total tokens method B: 793213\n",
            "Total tokens method C: 632485\n",
            "Total sentences: 60421\n",
            "Vocabulary size (unique tokens): 76551\n",
            "Average sentence length (in tokens): 10.47\n",
            "\n",
            "Language-based token counts (by sentence prediction):\n",
            "Spanish -> tokens: 12545, sentences: 5517\n",
            "Guarani -> tokens: 619911, sentences: 54896\n",
            "Other/unknown languages -> tokens: 30, sentences: 6\n",
            "Top 10 most common other languages:\n",
            "Unknown language [por]: 5\n",
            "Unknown language [eng]: 1\n",
            "\n",
            "Top 10 most common tokens:\n",
            "ha: 27318\n",
            "la: 9045\n",
            "hag̃ua: 7291\n",
            "pe: 7191\n",
            "umi: 6657\n",
            "che: 6601\n",
            "peteĩ: 5330\n",
            "mba’e: 4633\n",
            "avei: 4321\n",
            "de: 3958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# '/content/drive/MyDrive/Research/BID-GuaranIA/TXT COREGUAPA'"
      ],
      "metadata": {
        "id": "adFQ6jvaUIx7"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}